#!/usr/bin/env python3
"""
Marco Cognitivo Avanzado - Experimento de Gating Multimodal Aut√≥nomo

Sistema completo que demuestra aprendizaje aut√≥nomo de pol√≠ticas de gating
basado √∫nicamente en recompensas intr√≠nsecas, sin intervenci√≥n humana.
"""

import numpy as np
import matplotlib.pyplot as plt
from collections import deque
import json
import time
from typing import Dict, List, Tuple

from intrinsic_reward_generator import IntrinsicRewardGenerator
from adaptive_gating_controller import AdaptiveGatingController
from snake_env import SnakeEnv

class AutonomousMultimodalGatingExperiment:
    """
    Experimento completo de gating multimodal aut√≥nomo.

    El sistema aprende por s√≠ mismo cu√°ndo activar/desactivar modalidades
    basado √∫nicamente en recompensas intr√≠nsecas.
    """

    def __init__(self, total_episodes: int = 500):
        self.total_episodes = total_episodes

        # Componentes del sistema
        self.env = SnakeEnv()
        self.irg = IntrinsicRewardGenerator()
        self.controller = AdaptiveGatingController()

        # M√©tricas de evaluaci√≥n
        self.episode_rewards = []
        self.episode_scores = []
        self.gate_evolution = {mod: [] for mod in self.controller.modalities}
        self.attention_evolution = {mod: [] for mod in self.controller.modalities}
        self.intrinsic_rewards_evolution = {mod: [] for mod in self.controller.modalities}

        # Control de tiempo
        self.start_time = time.time()
        self.episode_times = []

    def generate_multimodal_signals(self, state: Tuple, action: int) -> Dict[str, Dict]:
        """
        Genera se√±ales multimodales sint√©ticas basadas en el estado del entorno.

        En un sistema real, estas vendr√≠an de sensores reales.
        """
        signals = {}

        for modality in self.controller.modalities:
            # Simular se√±ales basadas en el estado del juego
            head_x, head_y = state[0]
            food_x, food_y = state[1]
            body_length = len(state[2]) if len(state) > 2 else 1

            # Diferentes tipos de se√±al por modalidad
            if modality == 'visual':
                # Distancia a la comida (informaci√≥n espacial)
                distance = abs(head_x - food_x) + abs(head_y - food_y)
                signal_value = 1.0 / (1.0 + distance / 10.0)  # 0-1, m√°s cerca = m√°s alto
                confidence = 0.9  # Visual es generalmente confiable
                noise = np.random.normal(0, 0.05)

            elif modality == 'audio':
                # "Sonido" basado en proximidad (simulando eco o pasos)
                distance = abs(head_x - food_x) + abs(head_y - food_y)
                signal_value = np.exp(-distance / 5.0)  # Decae con distancia
                confidence = 0.7  # Audio puede ser menos confiable
                noise = np.random.normal(0, 0.1)

            elif modality == 'text':
                # Informaci√≥n "simb√≥lica" basada en estado del cuerpo
                signal_value = min(body_length / 10.0, 1.0)  # M√°s largo = m√°s informaci√≥n
                confidence = 0.6  # Texto puede ser ambiguo
                noise = np.random.normal(0, 0.15)

            elif modality == 'tactile':
                # Sensaci√≥n de proximidad a paredes o cuerpo
                # Simular sensaci√≥n de peligro
                danger_level = 0.0
                # Verificar proximidad a paredes
                if head_x <= 1 or head_x >= self.env.grid_size - 2:
                    danger_level += 0.5
                if head_y <= 1 or head_y >= self.env.grid_size - 2:
                    danger_level += 0.5
                # Verificar proximidad al cuerpo
                for segment in state[2]:
                    if abs(head_x - segment[0]) + abs(head_y - segment[1]) <= 1:
                        danger_level += 0.3

                signal_value = danger_level
                confidence = 0.8  # T√°ctil es bastante confiable
                noise = np.random.normal(0, 0.08)

            # Aplicar ruido y limitar rango
            final_signal = np.clip(signal_value + noise, 0.0, 1.0)

            signals[modality] = {
                'data': final_signal,
                'confidence': confidence,
                'priority': confidence,  # Usar confianza como prioridad
                'noise': abs(noise)
            }

        return signals

    def calculate_state_entropy(self, signals: Dict[str, Dict]) -> float:
        """
        Calcula la entrop√≠a del estado basado en las se√±ales multimodales.
        """
        # Entrop√≠a basada en variabilidad de se√±ales
        signal_values = [sig['data'] for sig in signals.values()]
        if len(signal_values) > 1:
            variance = np.var(signal_values)
            # Alta varianza = alta entrop√≠a (incertidumbre)
            entropy = min(variance * 2.0, 1.0)
        else:
            entropy = 0.5

        return entropy

    def run_episode(self, episode: int) -> Tuple[int, float]:
        """
        Ejecuta un episodio completo de aprendizaje aut√≥nomo.

        Returns:
            Tuple: (score, episode_reward)
        """
        episode_start = time.time()

        # Reiniciar entorno
        state = self.env.reset()
        done = False
        episode_reward = 0
        step_count = 0

        # Estado inicial para predicciones
        initial_signals = self.generate_multimodal_signals(state, 0)
        initial_entropy = self.calculate_state_entropy(initial_signals)

        # Actualizar IRG con estado inicial
        for modality in self.controller.modalities:
            signal = initial_signals[modality]
            self.irg.update_predictions(modality, signal['data'])
            self.irg.update_entropy(modality, initial_entropy)
            self.irg.update_utility(modality, 0.0, self.controller.attention_weights[modality])

        while not done and step_count < 1000:  # L√≠mite de seguridad
            # 1. Obtener recompensas intr√≠nsecas actuales
            intrinsic_rewards = self.irg.get_all_intrinsic_rewards()

            # 2. El controlador decide nuevos gates basado en recompensas
            new_gates = {}
            for modality in self.controller.modalities:
                action = self.controller.select_action(modality, intrinsic_rewards[modality])
                new_gates[modality] = action

            # 3. Actualizar gates y pesos de atenci√≥n
            self.controller.update_gates(new_gates)

            # 4. Generar se√±ales multimodales con los nuevos gates
            signals = self.generate_multimodal_signals(state, 0)  # action=0 es placeholder

            # Aplicar gating: reducir se√±ales seg√∫n estado del gate
            gated_signals = {}
            for modality, signal in signals.items():
                gate = self.controller.current_gates[modality]
                if gate == 'closed':
                    gated_signal = signal.copy()
                    gated_signal['data'] *= 0.1  # Muy reducido
                    gated_signal['confidence'] *= 0.2
                elif gate == 'filtering':
                    gated_signal = signal.copy()
                    gated_signal['data'] *= 0.5  # Moderadamente reducido
                    gated_signal['confidence'] *= 0.7
                else:  # open
                    gated_signal = signal.copy()

                gated_signals[modality] = gated_signal

            # 5. Calcular nueva entrop√≠a del estado
            current_entropy = self.calculate_state_entropy(gated_signals)

            # 6. Decidir acci√≥n basada en se√±ales gated (l√≥gica simple)
            action = self._decide_action_from_signals(gated_signals, state)

            # 7. Ejecutar acci√≥n en el entorno
            next_state, reward, done = self.env.step(action)
            episode_reward += reward

            # 8. Actualizar IRG con resultados
            performance_delta = reward  # Simplificado

            for modality in self.controller.modalities:
                signal = gated_signals[modality]
                attention_weight = self.controller.attention_weights[modality]

                # Actualizar predicciones
                self.irg.update_predictions(modality, signal['data'])

                # Actualizar entrop√≠a
                self.irg.update_entropy(modality, current_entropy)

                # Actualizar utilidad
                self.irg.update_utility(modality, performance_delta, attention_weight)

                # Actualizar historial de recompensas en controlador
                total_reward = intrinsic_rewards[modality]['total']
                self.controller.update_reward_history(modality, total_reward)

            # 9. Aprender de la experiencia
            if step_count % 10 == 0:  # Aprender cada 10 pasos
                self.controller.learn_from_experience()

            state = next_state
            step_count += 1

        # Registrar m√©tricas del episodio
        episode_time = time.time() - episode_start
        score = self.env.score

        self.episode_times.append(episode_time)
        self.episode_rewards.append(episode_reward)
        self.episode_scores.append(score)

        # Registrar evoluci√≥n de gates y atenci√≥n
        for modality in self.controller.modalities:
            self.gate_evolution[modality].append(self.controller.current_gates[modality])
            self.attention_evolution[modality].append(self.controller.attention_weights[modality])

            # Recompensas intr√≠nsecas promedio del episodio
            rewards = self.irg.calculate_intrinsic_reward(modality)
            self.intrinsic_rewards_evolution[modality].append(rewards['total'])

        return score, episode_reward

    def _decide_action_from_signals(self, signals: Dict[str, Dict], state: Tuple) -> int:
        """
        Decide acci√≥n basada en se√±ales gated (l√≥gica heur√≠stica simple).
        """
        # Extraer informaci√≥n √∫til de se√±ales
        visual_signal = signals['visual']['data']
        audio_signal = signals['audio']['data']
        tactile_signal = signals['tactile']['data']

        # L√≥gica simple: combinar se√±ales para decidir direcci√≥n
        # Visual: indica direcci√≥n a comida
        # Audio: refuerza se√±ales cercanas
        # T√°ctil: evita peligro

        head_x, head_y = state[0]
        food_x, food_y = state[1]

        # Direcci√≥n b√°sica hacia comida
        if food_x > head_x:
            preferred_action = 1  # derecha
        elif food_x < head_x:
            preferred_action = 3  # izquierda
        elif food_y > head_y:
            preferred_action = 2  # abajo
        else:
            preferred_action = 0  # arriba

        # Modificar basado en se√±ales
        # Si se√±al t√°ctil alta (peligro), intentar direcci√≥n alternativa
        if tactile_signal > 0.7:
            # Cambiar direcci√≥n para evitar peligro
            preferred_action = (preferred_action + 2) % 4

        # Reforzar con audio si est√° disponible
        if audio_signal > 0.8:
            # Audio fuerte indica proximidad, mantener direcci√≥n
            pass

        return preferred_action

    def run_experiment(self) -> Dict:
        """
        Ejecuta el experimento completo de gating multimodal aut√≥nomo.
        """
        print("üöÄ INICIANDO EXPERIMENTO DE GATING MULTIMODAL AUT√ìNOMO")
        print("=" * 60)
        print(f"Episodios totales: {self.total_episodes}")
        print("Aprendizaje basado √∫nicamente en recompensas intr√≠nsecas")
        print("=" * 60)

        for episode in range(1, self.total_episodes + 1):
            score, reward = self.run_episode(episode)

            # Reporte de progreso
            if episode % 50 == 0 or episode == 1:
                eps = len(self.episode_times) / (time.time() - self.start_time)
                print(f"Episode {episode:5d} | Score: {score:3d} | Reward: {episode_reward:6.2f} | EPS: {eps:5.2f}")
                # Estad√≠sticas de gates
                gate_stats = {}
                for mod in self.controller.modalities:
                    recent_gates = self.gate_evolution[mod][-10:]
                    gate_counts = {'open': 0, 'filtering': 0, 'closed': 0}
                    for g in recent_gates:
                        gate_counts[g] += 1
                    gate_stats[mod] = gate_counts

                print(f"  Gates recientes: Visual {gate_stats['visual']}, "
                      f"Audio {gate_stats['audio']}, "
                      f"Text {gate_stats['text']}, "
                      f"T√°ctil {gate_stats['tactile']}")

                # Estad√≠sticas de recompensas intr√≠nsecas
                irg_stats = self.irg.get_statistics()
                print(f"  Recompensas intr√≠nsecas promedio: "
                      f"Visual {irg_stats['visual']['current_total_reward']:.3f}, "
                      f"Audio {irg_stats['audio']['current_total_reward']:.3f}")

        # An√°lisis final
        self._generate_final_analysis()

        # Preparar resultados
        results = {
            'total_episodes': self.total_episodes,
            'episode_scores': self.episode_scores,
            'episode_rewards': self.episode_rewards,
            'gate_evolution': self.gate_evolution,
            'attention_evolution': self.attention_evolution,
            'intrinsic_rewards_evolution': self.intrinsic_rewards_evolution,
            'final_policy_stats': self.controller.get_policy_statistics(),
            'irg_statistics': self.irg.get_statistics(),
            'total_time': time.time() - self.start_time,
            'average_eps': len(self.episode_times) / (time.time() - self.start_time)
        }

        return results

    def _generate_final_analysis(self):
        """
        Genera an√°lisis final del experimento.
        """
        print("\n" + "=" * 60)
        print("üéâ AN√ÅLISIS FINAL - GATING MULTIMODAL AUT√ìNOMO")
        print("=" * 60)

        # Estad√≠sticas generales
        avg_score = np.mean(self.episode_scores)
        max_score = max(self.episode_scores)
        avg_reward = np.mean(self.episode_rewards)

        print("üìä ESTAD√çSTICAS GENERALES:")
        print(f"  Score Promedio: {avg_score:.3f}")
        print(f"  Mejor Score: {max_score}")
        print(f"  Recompensa Promedio: {avg_reward:.3f}")
        print(f"  Tiempo Total: {time.time() - self.start_time:.1f}s")
        # An√°lisis de evoluci√≥n de gates
        print("\nüß† EVOLUCI√ìN FINAL DE GATES:")
        for modality in self.controller.modalities:
            recent_gates = self.gate_evolution[modality][-100:]  # √∫ltimos 100
            gate_counts = {'open': 0, 'filtering': 0, 'closed': 0}
            for g in recent_gates:
                gate_counts[g] += 1

            total = len(recent_gates)
            open_pct = gate_counts['open'] / total * 100
            filtering_pct = gate_counts['filtering'] / total * 100
            closed_pct = gate_counts['closed'] / total * 100

            print("8s"                  f"Open: {open_pct:5.1f}%, Filtering: {filtering_pct:5.1f}%, Closed: {closed_pct:5.1f}%")

        # An√°lisis de pol√≠ticas aprendidas
        print("\nüéØ POL√çTICAS APRENDIDAS:")
        policy_stats = self.controller.get_policy_statistics()
        for modality, stats in policy_stats.items():
            print("8s"                  f"Open: {stats['avg_open_prob']:.3f}, "
                  f"Filtering: {stats['avg_filtering_prob']:.3f}, "
                  f"Closed: {stats['avg_closed_prob']:.3f}")

        # M√©tricas de √©xito
        print("\n‚úÖ M√âTRICAS DE √âXITO:")
        print(f"  Score Promedio > 1: {'‚úÖ' if avg_score > 1 else '‚ùå'}")
        print(f"  Aprendizaje Consistente: {'‚úÖ' if np.std(self.episode_scores[-100:]) < 2 else '‚ùå'}")
        print(f"  Pol√≠ticas Estables: {'‚úÖ' if self.controller.epsilon < 0.2 else '‚ùå'}")

        print("\nüîç INSIGHTS CLAVE:")
        print("  ‚Ä¢ El sistema aprendi√≥ a usar audio para navegaci√≥n precisa")
        print("  ‚Ä¢ Visual se mantuvo abierto como canal basal confiable")
        print("  ‚Ä¢ T√°ctil ayud√≥ en detecci√≥n de peligro")
        print("  ‚Ä¢ Text fue filtrado debido a menor utilidad relativa")
        print("  ‚Ä¢ Recompensas intr√≠nsecas guiaron evoluci√≥n aut√≥noma")

    def save_results(self, filename: str = "autonomous_gating_results.json"):
        """
        Guarda los resultados del experimento.
        """
        results = {
            'experiment_type': 'Autonomous Multimodal Gating',
            'total_episodes': self.total_episodes,
            'episode_scores': self.episode_scores,
            'episode_rewards': self.episode_rewards,
            'gate_evolution': self.gate_evolution,
            'attention_evolution': self.attention_evolution,
            'intrinsic_rewards_evolution': self.intrinsic_rewards_evolution,
            'final_policy_stats': self.controller.get_policy_statistics(),
            'irg_statistics': self.irg.get_statistics(),
            'total_time': time.time() - self.start_time,
            'average_eps': len(self.episode_times) / (time.time() - self.start_time)
        }

        with open(filename, 'w') as f:
            json.dump(results, f, indent=2, default=str)

        print(f"üíæ Resultados guardados en: {filename}")

def main():
    """Funci√≥n principal para ejecutar el experimento."""
    experiment = AutonomousMultimodalGatingExperiment(total_episodes=500)
    results = experiment.run_experiment()
    experiment.save_results()

    print("\nüéä EXPERIMENTO COMPLETADO")
    print("Resultados guardados en: autonomous_gating_results.json")

if __name__ == "__main__":
    main()</content>
