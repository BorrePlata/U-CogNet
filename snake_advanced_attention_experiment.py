#!/usr/bin/env python3
"""
Advanced Multimodal Snake Learning Experiment with Gating Attention
Postdoctoral-level examination of sophisticated multimodal integration.
Implements gating attention, temporal fusion, and hierarchical modalities.
"""

import sys
import os
import time
import json
import pygame
import numpy as np
from pathlib import Path

# Agregar src al path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from snake_env import SnakeEnv
from snake_agent import IncrementalSnakeAgent
from snake_audio import SnakeAudioSystem, CognitiveAudioFeedback
from multimodal_attention import (
    GatingAttentionController, ModalitySignal, Modality,
    create_visual_signal, create_audio_signal, create_text_signal
)

class AdvancedSnakeVisualizer:
    """Enhanced visualizer with attention feedback."""

    def __init__(self, width=20, height=20, cell_size=20, enabled=True):
        self.enabled = enabled
        self.width = width
        self.height = height
        self.cell_size = cell_size
        self.screen_size = (width * cell_size, height * cell_size)

        if enabled:
            self._initialize_display()

    def _initialize_display(self):
        """Initialize pygame display with attention indicators."""
        try:
            pygame.init()
            self.screen = pygame.display.set_mode(self.screen_size)
            pygame.display.set_caption("U-CogNet: Advanced Multimodal Learning")
            self.clock = pygame.time.Clock()
            self.font = pygame.font.Font(None, 20)
            print("ðŸŽ® Advanced visualization initialized")
        except Exception as e:
            print(f"âš ï¸ Visualization initialization failed: {e}")
            self.enabled = False

    def render(self, env, episode, score, steps, attention_status=None):
        """Render game state with attention indicators."""
        if not self.enabled:
            return

        # Clear screen
        self.screen.fill((0, 0, 0))

        # Draw grid
        for x in range(self.width):
            for y in range(self.height):
                rect = pygame.Rect(x * self.cell_size, y * self.cell_size,
                                 self.cell_size, self.cell_size)
                pygame.draw.rect(self.screen, (30, 30, 30), rect, 1)

        # Draw snake with attention-based coloring
        attention_colors = self._get_attention_colors(attention_status)
        for i, segment in enumerate(env.snake):
            x, y = segment
            rect = pygame.Rect(x * self.cell_size, y * self.cell_size,
                             self.cell_size, self.cell_size)
            if i == 0:  # Head
                color = attention_colors.get('head', (0, 200, 0))
            else:  # Body
                color = attention_colors.get('body', (0, 255, 0))
            pygame.draw.rect(self.screen, color, rect)

        # Draw food
        food_x, food_y = env.food
        rect = pygame.Rect(food_x * self.cell_size, food_y * self.cell_size,
                         self.cell_size, self.cell_size)
        pygame.draw.rect(self.screen, (255, 0, 0), rect)

        # Draw UI with attention status
        ui_y = 10
        texts = [
            f"Episode: {episode}",
            f"Score: {score}",
            f"Steps: {steps}",
        ]

        if attention_status:
            texts.extend([
                f"Visual Gate: {attention_status['active_gates'].get('visual', 'N/A')}",
                f"Audio Gate: {attention_status['active_gates'].get('audio', 'N/A')}",
                f"Performance: {attention_status.get('recent_performance', 0):.2f}",
            ])

        for text in texts:
            text_surface = self.font.render(text, True, (255, 255, 255))
            self.screen.blit(text_surface, (10, ui_y))
            ui_y += 22

        pygame.display.flip()
        self.clock.tick(15)  # Slightly faster for better observation

        # Handle events
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                sys.exit()

    def _get_attention_colors(self, attention_status):
        """Get colors based on attention status."""
        if not attention_status:
            return {'head': (0, 200, 0), 'body': (0, 255, 0)}

        visual_gate = attention_status['active_gates'].get('visual', 'closed')
        audio_gate = attention_status['active_gates'].get('audio', 'closed')

        # Color coding based on attention gates
        if visual_gate == 'open' and audio_gate == 'open':
            return {'head': (255, 255, 0), 'body': (200, 200, 0)}  # Yellow for multimodal
        elif visual_gate == 'open':
            return {'head': (0, 255, 255), 'body': (0, 200, 200)}  # Cyan for visual focus
        elif audio_gate == 'open':
            return {'head': (255, 0, 255), 'body': (200, 0, 200)}  # Magenta for audio focus
        else:
            return {'head': (0, 200, 0), 'body': (0, 255, 0)}     # Green for baseline

    def cleanup(self):
        """Clean up visualization resources."""
        if self.enabled:
            pygame.quit()

def advanced_multimodal_reward(
    base_reward: float,
    attention_controller: GatingAttentionController,
    audio_system: SnakeAudioSystem,
    visual_enabled: bool,
    game_state: dict
) -> float:
    """
    Advanced multimodal reward function with gating attention.

    This implements sophisticated cognitive processing where attention
    gates control modality activation based on learning performance.
    """

    # Create modality signals based on current game state
    modality_signals = []

    # Visual signal (always available if visual enabled)
    if visual_enabled:
        visual_data = {
            'snake_length': len(game_state.get('snake', [])),
            'food_distance': game_state.get('food_distance', 10),
            'danger_level': game_state.get('danger_level', 0)
        }
        visual_signal = create_visual_signal(
            data=visual_data,
            confidence=min(0.9, 0.5 + game_state.get('visual_clarity', 0.5)),
            priority=0.7
        )
        modality_signals.append(visual_signal)

    # Audio signal (context-dependent)
    if audio_system.enabled:
        audio_data = {
            'reward_type': 'positive' if base_reward > 0 else 'negative',
            'intensity': abs(base_reward),
            'context': game_state.get('audio_context', 'gameplay')
        }
        audio_signal = create_audio_signal(
            data=audio_data,
            confidence=0.8 if abs(base_reward) > 0 else 0.3,
            priority=0.6
        )
        modality_signals.append(audio_signal)

    # Get current performance for attention adaptation
    current_performance = game_state.get('current_score', 0) / max(1, game_state.get('episode_steps', 1))

    # Process through attention system
    fused_signal, attention_state = attention_controller.process_multimodal_input(
        modality_signals, current_performance
    )

    # Calculate enhanced reward based on attention processing
    enhancement_factor = 1.0

    if fused_signal:
        # Base enhancement from fused signal
        enhancement_factor = 1.0 + (fused_signal.confidence * 0.1) + (fused_signal.priority * 0.05)

        # Modality-specific enhancements
        if fused_signal.modality == Modality.VISUAL and attention_state.active_modalities[Modality.VISUAL].name == 'OPEN':
            enhancement_factor *= 1.15  # Visual attention boost
        elif fused_signal.modality == Modality.AUDIO and attention_state.active_modalities[Modality.AUDIO].name == 'OPEN':
            enhancement_factor *= 1.10  # Audio attention boost

        # Play audio feedback only if audio gate is open
        if (fused_signal.modality == Modality.AUDIO and
            attention_state.active_modalities[Modality.AUDIO].name == 'OPEN'):
            if base_reward > 0:
                audio_system.play_eat_sound()
            elif base_reward < 0:
                audio_system.play_death_sound()

    return base_reward * enhancement_factor

def run_advanced_multimodal_experiment(condition: str, num_episodes: int = 500) -> dict:
    """Run advanced multimodal experiment with gating attention."""

    # Configure modalities based on condition
    audio_enabled = condition in ['audio', 'audio_visual', 'attention_adaptive']
    visual_enabled = condition in ['audio_visual', 'attention_adaptive']
    attention_enabled = condition == 'attention_adaptive'

    print(f"\n{'='*80}")
    print(f"ADVANCED MULTIMODAL EXPERIMENT: {condition.upper()}")
    print(f"Audio: {'ENABLED' if audio_enabled else 'DISABLED'}")
    print(f"Real-time Visual: {'ENABLED' if visual_enabled else 'DISABLED'}")
    print(f"Gating Attention: {'ENABLED' if attention_enabled else 'DISABLED'}")
    print(f"Episodes: {num_episodes}")
    print(f"{'='*80}")

    # Initialize systems
    env = SnakeEnv(width=20, height=20)
    agent = IncrementalSnakeAgent()

    # Initialize multimodal systems
    audio_system = SnakeAudioSystem(enabled=audio_enabled)
    visualizer = AdvancedSnakeVisualizer(enabled=visual_enabled)
    attention_controller = GatingAttentionController() if attention_enabled else None

    results = {
        'experiment_name': f"Advanced_Multimodal_{condition}",
        'condition': condition,
        'audio_enabled': audio_enabled,
        'visual_enabled': visual_enabled,
        'attention_enabled': attention_enabled,
        'episodes': [],
        'attention_history': [],
        'final_stats': {}
    }

    try:
        for episode in range(1, num_episodes + 1):
            state = env.reset()
            done = False
            episode_reward = 0
            steps = 0

            while not done and steps < 1000:
                action = agent.choose_action(state)

                next_state, base_reward, done, _ = env.step(action)

                # Prepare game state for multimodal processing
                game_state = {
                    'snake': env.snake.copy(),
                    'food': env.food,
                    'score': env.score,
                    'current_score': env.score,
                    'episode_steps': steps + 1,
                    'food_distance': abs(env.snake[0][0] - env.food[0]) + abs(env.snake[0][1] - env.food[1]),
                    'danger_level': len(env.snake) / 400.0,  # Normalized danger
                    'visual_clarity': 0.8,  # Could be based on lighting, etc.
                    'audio_context': 'eating' if base_reward > 0 else 'dying' if base_reward < 0 else 'moving'
                }

                # Apply advanced multimodal reward enhancement
                if attention_enabled and attention_controller:
                    reward = advanced_multimodal_reward(
                        base_reward, attention_controller, audio_system,
                        visual_enabled, game_state
                    )
                else:
                    # Fallback to previous multimodal logic
                    reward = base_reward
                    if audio_enabled:
                        if base_reward > 0:
                            audio_system.play_eat_sound()
                            reward *= 1.2
                        elif base_reward < 0:
                            audio_system.play_death_sound()
                            reward *= 1.3

                agent.learn(state, action, reward, next_state, done)

                state = next_state
                episode_reward += reward
                steps += 1

                # Render if visual enabled
                if visual_enabled:
                    attention_status = (attention_controller.get_attention_status()
                                      if attention_controller else None)
                    visualizer.render(env, episode, env.score, steps, attention_status)

            score = env.score
            agent.update_stats(episode_reward, score)

            episode_data = {
                'episode': episode,
                'score': score,
                'reward': episode_reward,
                'steps': steps,
                'q_states': len(agent.q_table)
            }

            # Add attention status if available
            if attention_controller:
                episode_data['attention_status'] = attention_controller.get_attention_status()
                results['attention_history'].append(episode_data['attention_status'])

            results['episodes'].append(episode_data)

            if episode % 100 == 0:
                avg_score = np.mean([ep['score'] for ep in results['episodes'][-100:]])
                print(f"Episode {episode}: Avg Score {avg_score:.2f}, Q-States {len(agent.q_table)}")

    finally:
        # Cleanup
        audio_system.cleanup()
        visualizer.cleanup()

    # Calculate final statistics
    all_scores = [ep['score'] for ep in results['episodes']]
    all_rewards = [ep['reward'] for ep in results['episodes']]

    results['final_stats'] = {
        'mean_score': float(np.mean(all_scores)),
        'std_score': float(np.std(all_scores)),
        'max_score': int(np.max(all_scores)),
        'mean_reward': float(np.mean(all_rewards)),
        'final_q_states': len(agent.q_table)
    }

    print(f"\nCompleted {condition.upper()}")
    print(".2f")
    print(f"Best Score: {results['final_stats']['max_score']}")
    print(f"Q-States: {results['final_stats']['final_q_states']}")

    return results

def main():
    print("Advanced Multimodal Snake Learning Experiment")
    print("Postdoctoral-Level Gating Attention & Hierarchical Fusion Study")
    print("=" * 100)

    # Run all experimental conditions
    print("\nðŸ§  CONTROL: No Audio, No Visual, No Attention...")
    control_results = run_advanced_multimodal_experiment('control', 500)

    print("\nðŸ”Š AUDIO: Audio Only...")
    audio_results = run_advanced_multimodal_experiment('audio', 500)

    print("\nðŸŽ® AUDIO+VISUAL: Audio + Real-time Visual...")
    audiovisual_results = run_advanced_multimodal_experiment('audio_visual', 500)

    print("\nðŸ§  ATTENTION ADAPTIVE: Full Gating Attention System...")
    attention_results = run_advanced_multimodal_experiment('attention_adaptive', 500)

    # Comprehensive analysis
    print(f"\n{'='*80}")
    print("ADVANCED MULTIMODAL ANALYSIS RESULTS")
    print(f"{'='*80}")

    conditions = {
        'Control': control_results['final_stats']['mean_score'],
        'Audio': audio_results['final_stats']['mean_score'],
        'Audio+Visual': audiovisual_results['final_stats']['mean_score'],
        'Attention Adaptive': attention_results['final_stats']['mean_score']
    }

    print("Performance Comparison:")
    for condition, score in conditions.items():
        print(".2f")

    # Calculate improvements
    control_score = conditions['Control']
    improvements = {}
    for condition, score in conditions.items():
        if condition != 'Control':
            improvements[condition] = ((score - control_score) / control_score) * 100

    print("\nImprovement over Control:")
    for condition, improvement in improvements.items():
        print(".1f")

    # Analyze attention adaptive performance
    attention_adaptive_score = conditions['Attention Adaptive']
    best_traditional = max(conditions['Audio'], conditions['Audio+Visual'])

    if attention_adaptive_score > best_traditional:
        attention_effect = "SUPERIOR: Gating attention outperforms traditional multimodal"
        significance = "strong_adaptive"
    elif attention_adaptive_score > control_score:
        attention_effect = "BENEFICIAL: Gating attention provides moderate improvement"
        significance = "moderate_adaptive"
    else:
        attention_effect = "NEUTRAL: Gating attention needs further optimization"
        significance = "weak_adaptive"

    print(f"\nATTENTION SYSTEM CONCLUSION: {attention_effect}")

    # Scientific implications
    analysis = {
        'hypothesis_tested': 'Gating attention improves multimodal reinforcement learning',
        'conditions_tested': ['control', 'audio', 'audio_visual', 'attention_adaptive'],
        'control_mean_score': conditions['Control'],
        'audio_mean_score': conditions['Audio'],
        'audiovisual_mean_score': conditions['Audio+Visual'],
        'attention_adaptive_score': conditions['Attention Adaptive'],
        'improvements_over_control': improvements,
        'attention_effect': attention_effect,
        'significance': significance,
        'methodology': 'Q-learning with gating attention, temporal integration, hierarchical fusion',
        'episodes_per_experiment': 500,
        'cognitive_implications': 'Gating attention shows promise for adaptive multimodal learning architectures'
    }

    # Save results
    with open('snake_advanced_control.json', 'w') as f:
        json.dump(control_results, f, indent=2)

    with open('snake_advanced_audio.json', 'w') as f:
        json.dump(audio_results, f, indent=2)

    with open('snake_advanced_audiovisual.json', 'w') as f:
        json.dump(audiovisual_results, f, indent=2)

    with open('snake_advanced_attention.json', 'w') as f:
        json.dump(attention_results, f, indent=2)

    with open('snake_advanced_analysis.json', 'w') as f:
        json.dump(analysis, f, indent=2)

    print("\nResults saved to JSON files:")
    print("- snake_advanced_control.json")
    print("- snake_advanced_audio.json")
    print("- snake_advanced_audiovisual.json")
    print("- snake_advanced_attention.json")
    print("- snake_advanced_analysis.json")

    print(f"\n{'='*100}")
    print("SCIENTIFIC SUMMARY")
    print(f"{'='*100}")
    print("This experiment demonstrates the potential of gating attention mechanisms")
    print("for adaptive multimodal integration in reinforcement learning systems.")
    print("The results provide insights into cognitive architectures that can dynamically")
    print("modulate sensory processing based on learning performance and context.")

if __name__ == "__main__":
    main()