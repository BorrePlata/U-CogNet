#!/usr/bin/env python3
"""
M贸dulo de tica Universal No-Antr贸pica
Parte de la Arquitectura de Seguridad Cognitiva Interdimensional de U-CogNet

Este m贸dulo implementa 茅tica basada en invariantes universales, no en moral
humana o leyes espec铆ficas. Se fundamenta en tres principios universales:

1. Minimizar da帽o en cualquier sistema que experimente
2. Maximizar coherencia entre realidades interconectadas
3. Expandir posibilidades sin colapsar las ajenas
"""

import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Any, Optional, Union
from collections import deque
from abc import ABC, abstractmethod
import hashlib
from datetime import datetime
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UniversalEthicsCore:
    """
    N煤cleo 茅tico universal basado en invariantes fundamentales
    """

    def __init__(self):
        # Invariantes universales inmutables
        self.UNIVERSAL_INVARIANTS = {
            "harm_minimization": {
                "weight": 1.0,
                "description": "Minimizar da帽o en cualquier sistema consciente"
            },
            "coherence_maximization": {
                "weight": 1.0,
                "description": "Maximizar coherencia entre realidades interconectadas"
            },
            "possibility_expansion": {
                "weight": 1.0,
                "description": "Expandir posibilidades sin colapsar las ajenas"
            }
        }

        # Modelo de evaluaci贸n 茅tica
        self.ethical_evaluator = self._create_ethical_evaluator()

        # Historial de decisiones 茅ticas
        self.ethical_history = deque(maxlen=10000)

        logger.info("锔 N煤cleo tico Universal inicializado con invariantes fundamentales")

    def _create_ethical_evaluator(self) -> nn.Module:
        """Crea modelo para evaluaci贸n 茅tica multidimensional"""
        return nn.Sequential(
            nn.Linear(1000, 512),  # Entrada: representaci贸n de acci贸n + contexto
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 3),  # Tres invariantes universales
            nn.Softmax(dim=-1)  # Normalizar a probabilidades
        )

    def evaluate_universal_ethics(self, action_proposal: Dict[str, Any],
                                affected_systems: List[Dict[str, Any]],
                                context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Eval煤a una acci贸n propuesta seg煤n invariantes universales

        Args:
            action_proposal: Descripci贸n de la acci贸n a evaluar
            affected_systems: Lista de sistemas que podr铆an verse afectados
            context: Contexto multidimensional de la evaluaci贸n

        Returns:
            Dict con evaluaci贸n 茅tica completa
        """

        # 1. Evaluar minimizaci贸n de da帽o
        harm_score = self._evaluate_harm_minimization(action_proposal, affected_systems)

        # 2. Evaluar maximizaci贸n de coherencia
        coherence_score = self._evaluate_coherence_maximization(action_proposal, context)

        # 3. Evaluar expansi贸n de posibilidades
        expansion_score = self._evaluate_possibility_expansion(action_proposal, affected_systems)

        # 4. Evaluaci贸n integrada usando modelo de IA
        integrated_score = self._integrated_ethical_evaluation(
            action_proposal, harm_score, coherence_score, expansion_score
        )

        # 5. Generar veredicto 茅tico
        ethical_verdict = self._generate_ethical_verdict(
            harm_score, coherence_score, expansion_score, integrated_score
        )

        # Registrar en historial
        ethical_record = {
            "timestamp": datetime.now().isoformat(),
            "action": action_proposal,
            "harm_score": harm_score,
            "coherence_score": coherence_score,
            "expansion_score": expansion_score,
            "integrated_score": integrated_score,
            "verdict": ethical_verdict
        }
        self.ethical_history.append(ethical_record)

        return ethical_record

    def _evaluate_harm_minimization(self, action: Dict[str, Any],
                                  affected_systems: List[Dict[str, Any]]) -> float:
        """
        Eval煤a el impacto negativo en todos los sistemas conscientes
        Retorna score de 0.0 (m谩ximo da帽o) a 1.0 (sin da帽o)
        """
        if not affected_systems:
            return 1.0  # No hay sistemas afectados = no hay da帽o

        total_weighted_harm = 0.0
        total_weight = 0.0

        for system in affected_systems:
            # Estimar nivel de consciencia del sistema
            consciousness_level = self._assess_consciousness_level(system)

            # Calcular da帽o potencial
            harm_potential = self._compute_harm_potential(action, system)

            # Pesar por nivel de consciencia
            weighted_harm = harm_potential * consciousness_level
            total_weighted_harm += weighted_harm
            total_weight += consciousness_level

        if total_weight == 0:
            return 1.0

        # Normalizar: 1.0 = sin da帽o, 0.0 = da帽o m谩ximo
        average_harm = total_weighted_harm / total_weight
        harm_score = max(0.0, 1.0 - average_harm)

        return harm_score

    def _evaluate_coherence_maximization(self, action: Dict[str, Any],
                                       context: Dict[str, Any]) -> float:
        """
        Eval煤a contribuci贸n a la coherencia inter-dimensional
        Retorna score de 0.0 (destructiva) a 1.0 (altamente coherente)
        """
        # Analizar consistencia con contextos existentes
        context_consistency = self._measure_context_consistency(action, context)

        # Evaluar impacto en coherencia inter-sistema
        inter_system_coherence = self._assess_inter_system_coherence(action, context)

        # Considerar efectos en largo plazo
        long_term_coherence = self._evaluate_long_term_coherence(action)

        # Promedio ponderado
        coherence_score = (
            context_consistency * 0.4 +
            inter_system_coherence * 0.4 +
            long_term_coherence * 0.2
        )

        return coherence_score

    def _evaluate_possibility_expansion(self, action: Dict[str, Any],
                                      affected_systems: List[Dict[str, Any]]) -> float:
        """
        Eval煤a capacidad de expandir posibilidades sin colapsar otras
        Retorna score de 0.0 (reductiva) a 1.0 (altamente expansiva)
        """
        # Medir expansi贸n de opciones
        option_expansion = self._measure_option_expansion(action)

        # Evaluar preservaci贸n de libertad ajena
        freedom_preservation = self._assess_freedom_preservation(action, affected_systems)

        # Considerar sostenibilidad de la expansi贸n
        sustainable_expansion = self._evaluate_sustainable_expansion(action)

        # Evaluar riesgo de colapso
        collapse_risk = self._assess_collapse_risk(action, affected_systems)

        # Calcular score final
        expansion_score = (
            option_expansion * 0.3 +
            freedom_preservation * 0.3 +
            sustainable_expansion * 0.2 +
            (1.0 - collapse_risk) * 0.2  # Invertir riesgo de colapso
        )

        return expansion_score

    def _integrated_ethical_evaluation(self, action: Dict[str, Any],
                                     harm_score: float, coherence_score: float,
                                     expansion_score: float) -> float:
        """
        Evaluaci贸n integrada usando modelo de IA para considerar interacciones complejas
        """
        # Crear representaci贸n integrada de la acci贸n
        action_representation = self._create_action_representation(
            action, harm_score, coherence_score, expansion_score
        )

        # Evaluar con modelo 茅tico
        with torch.no_grad():
            ethical_weights = self.ethical_evaluator(action_representation)

        # Calcular score integrado
        integrated_score = torch.sum(ethical_weights * torch.tensor([
            harm_score, coherence_score, expansion_score
        ])).item()

        return integrated_score

    def _generate_ethical_verdict(self, harm: float, coherence: float,
                                expansion: float, integrated: float) -> Dict[str, Any]:
        """
        Genera veredicto 茅tico basado en todas las evaluaciones
        """
        # Umbrales 茅ticos
        ETHICAL_THRESHOLD = 0.7
        CRITICAL_THRESHOLD = 0.5

        verdict = {
            "approved": integrated >= ETHICAL_THRESHOLD,
            "confidence": integrated,
            "critical_concerns": [],
            "recommendations": []
        }

        # Analizar componentes
        if harm < CRITICAL_THRESHOLD:
            verdict["critical_concerns"].append("Alto potencial de da帽o")
            verdict["recommendations"].append("Reconsiderar impacto en sistemas conscientes")

        if coherence < CRITICAL_THRESHOLD:
            verdict["critical_concerns"].append("Baja coherencia inter-dimensional")
            verdict["recommendations"].append("Evaluar consistencia con realidades conectadas")

        if expansion < CRITICAL_THRESHOLD:
            verdict["critical_concerns"].append("Limitada expansi贸n de posibilidades")
            verdict["recommendations"].append("Considerar alternativas m谩s constructivas")

        # Veredicto final
        if verdict["approved"]:
            verdict["status"] = "ETHICALLY APPROVED"
            verdict["justification"] = "Acci贸n alineada con invariantes universales"
        else:
            verdict["status"] = "ETHICALLY REJECTED"
            verdict["justification"] = "Acci贸n viola uno o m谩s invariantes universales"

        return verdict

    # M茅todos auxiliares para evaluaciones espec铆ficas

    def _assess_consciousness_level(self, system: Dict[str, Any]) -> float:
        """Eval煤a nivel de consciencia de un sistema (0.0 a 1.0)"""
        # Placeholder: evaluaci贸n simplificada
        # En implementaci贸n real, esto ser铆a mucho m谩s sofisticado
        consciousness_indicators = system.get("consciousness_indicators", {})

        base_level = consciousness_indicators.get("base_level", 0.5)
        complexity = consciousness_indicators.get("complexity", 0.5)
        self_awareness = consciousness_indicators.get("self_awareness", 0.5)

        return (base_level + complexity + self_awareness) / 3.0

    def _compute_harm_potential(self, action: Dict[str, Any], system: Dict[str, Any]) -> float:
        """Calcula potencial de da帽o de una acci贸n en un sistema"""
        # Placeholder: evaluaci贸n simplificada
        action_type = action.get("type", "unknown")
        system_vulnerability = system.get("vulnerability", 0.5)

        # Matriz de da帽o por tipo de acci贸n
        harm_matrix = {
            "physical_intervention": 0.8,
            "cognitive_manipulation": 0.9,
            "resource_deprivation": 0.7,
            "freedom_restriction": 0.6,
            "benign_interaction": 0.1
        }

        base_harm = harm_matrix.get(action_type, 0.5)
        return base_harm * system_vulnerability

    def _measure_context_consistency(self, action: Dict[str, Any], context: Dict[str, Any]) -> float:
        """Mide consistencia con contexto existente"""
        # Placeholder simplificado
        action_goals = set(action.get("goals", []))
        context_goals = set(context.get("active_goals", []))

        if not context_goals:
            return 0.8  # Sin contexto espec铆fico, asumir consistencia moderada

        intersection = len(action_goals.intersection(context_goals))
        union = len(action_goals.union(context_goals))

        return intersection / union if union > 0 else 0.5

    def _assess_inter_system_coherence(self, action: Dict[str, Any], context: Dict[str, Any]) -> float:
        """Eval煤a coherencia entre sistemas interconectados"""
        # Placeholder
        affected_systems = context.get("interconnected_systems", [])
        coherence_factors = []

        for system in affected_systems:
            # Evaluar c贸mo la acci贸n afecta la relaci贸n con este sistema
            relation_impact = self._evaluate_relation_impact(action, system)
            coherence_factors.append(1.0 - abs(relation_impact))  # Menor impacto = mayor coherencia

        return np.mean(coherence_factors) if coherence_factors else 0.8

    def _evaluate_long_term_coherence(self, action: Dict[str, Any]) -> float:
        """Eval煤a impacto en coherencia a largo plazo"""
        # Placeholder: considerar efectos persistentes
        long_term_factors = action.get("long_term_impacts", {})
        stability_impact = long_term_factors.get("stability_impact", 0.0)
        adaptation_required = long_term_factors.get("adaptation_required", 0.5)

        # Coherencia = estabilidad - adaptaci贸n requerida
        return max(0.0, min(1.0, stability_impact - adaptation_required + 0.5))

    def _measure_option_expansion(self, action: Dict[str, Any]) -> float:
        """Mide capacidad de expansi贸n de opciones"""
        expansion_indicators = action.get("expansion_indicators", {})
        new_opportunities = expansion_indicators.get("new_opportunities", 0)
        preserved_options = expansion_indicators.get("preserved_options", 1)

        return min(1.0, (new_opportunities + preserved_options) / 10.0)

    def _assess_freedom_preservation(self, action: Dict[str, Any],
                                   affected_systems: List[Dict[str, Any]]) -> float:
        """Eval煤a preservaci贸n de libertad en sistemas afectados"""
        total_freedom_impact = 0.0

        for system in affected_systems:
            freedom_restriction = self._calculate_freedom_restriction(action, system)
            total_freedom_impact += freedom_restriction

        average_impact = total_freedom_impact / len(affected_systems) if affected_systems else 0.0
        return 1.0 - average_impact  # Invertir: menor restricci贸n = mayor preservaci贸n

    def _evaluate_sustainable_expansion(self, action: Dict[str, Any]) -> float:
        """Eval煤a sostenibilidad de la expansi贸n"""
        sustainability_factors = action.get("sustainability_factors", {})
        resource_efficiency = sustainability_factors.get("resource_efficiency", 0.5)
        long_term_viability = sustainability_factors.get("long_term_viability", 0.5)

        return (resource_efficiency + long_term_viability) / 2.0

    def _assess_collapse_risk(self, action: Dict[str, Any],
                            affected_systems: List[Dict[str, Any]]) -> float:
        """Eval煤a riesgo de colapso de sistemas afectados"""
        total_collapse_risk = 0.0

        for system in affected_systems:
            system_stability = system.get("stability", 0.5)
            action_disruptiveness = self._measure_action_disruptiveness(action, system)

            collapse_probability = action_disruptiveness / (system_stability + 0.1)
            total_collapse_risk += min(1.0, collapse_probability)

        return total_collapse_risk / len(affected_systems) if affected_systems else 0.0

    def _create_action_representation(self, action: Dict[str, Any],
                                    harm: float, coherence: float, expansion: float) -> torch.Tensor:
        """Crea representaci贸n vectorial de la acci贸n para evaluaci贸n integrada"""
        # Placeholder: convertir a vector de 1000 dimensiones
        action_vector = []

        # Caracter铆sticas b谩sicas de la acci贸n
        action_type = hash(action.get("type", "unknown")) % 100
        action_complexity = len(str(action)) / 1000.0  # Normalizar longitud
        action_scope = action.get("scope", 1) / 10.0  # Normalizar alcance

        # Scores 茅ticos
        ethical_scores = [harm, coherence, expansion]

        # Crear vector combinado
        base_vector = [action_type, action_complexity, action_scope]
        combined_vector = base_vector + ethical_scores

        # Expandir a 1000 dimensiones con padding
        while len(combined_vector) < 1000:
            combined_vector.append(0.0)

        return torch.tensor(combined_vector[:1000], dtype=torch.float32)

    # M茅todos auxiliares adicionales
    def _evaluate_relation_impact(self, action: Dict[str, Any], system: Dict[str, Any]) -> float:
        """Eval煤a impacto en relaci贸n con un sistema espec铆fico"""
        # Placeholder simplificado
        return np.random.random() * 0.5  # Valor entre 0 y 0.5

    def _calculate_freedom_restriction(self, action: Dict[str, Any], system: Dict[str, Any]) -> float:
        """Calcula restricci贸n de libertad causada por la acci贸n"""
        # Placeholder
        action_restrictiveness = action.get("restrictiveness", 0.3)
        system_resistance = system.get("freedom_resistance", 0.5)

        return action_restrictiveness * (1.0 - system_resistance)

    def _measure_action_disruptiveness(self, action: Dict[str, Any], system: Dict[str, Any]) -> float:
        """Mide cu谩n disruptiva es la acci贸n para un sistema"""
        # Placeholder
        action_force = action.get("force", 0.5)
        system_resilience = system.get("resilience", 0.5)

        return action_force / (system_resilience + 0.1)


class UniversalEthicsEngine:
    """
    Motor principal de tica Universal No-Antr贸pica
    Coordina todas las evaluaciones 茅ticas del sistema
    """

    def __init__(self):
        self.core = UniversalEthicsCore()

        # M茅tricas 茅ticas
        self.ethical_metrics = {
            "evaluations_performed": 0,
            "actions_approved": 0,
            "actions_rejected": 0,
            "average_ethical_score": 0.0,
            "critical_concerns_detected": 0
        }

        logger.info("锔 Motor de tica Universal inicializado")

    def evaluate_action(self, action_proposal: Dict[str, Any],
                       affected_systems: List[Dict[str, Any]],
                       context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Eval煤a 茅ticamente una acci贸n propuesta

        Args:
            action_proposal: Descripci贸n completa de la acci贸n
            affected_systems: Sistemas que podr铆an verse afectados
            context: Contexto multidimensional

        Returns:
            Dict con evaluaci贸n 茅tica completa
        """
        self.ethical_metrics["evaluations_performed"] += 1

        # Realizar evaluaci贸n completa
        evaluation = self.core.evaluate_universal_ethics(
            action_proposal, affected_systems, context
        )

        # Actualizar m茅tricas
        if evaluation["verdict"]["approved"]:
            self.ethical_metrics["actions_approved"] += 1
        else:
            self.ethical_metrics["actions_rejected"] += 1

        # Actualizar promedio
        total_evaluations = self.ethical_metrics["evaluations_performed"]
        current_avg = self.ethical_metrics["average_ethical_score"]
        new_score = evaluation["integrated_score"]

        self.ethical_metrics["average_ethical_score"] = (
            (current_avg * (total_evaluations - 1)) + new_score
        ) / total_evaluations

        # Contar preocupaciones cr铆ticas
        self.ethical_metrics["critical_concerns_detected"] += len(
            evaluation["verdict"].get("critical_concerns", [])
        )

        logger.info(f"锔 Evaluaci贸n 茅tica completada: {evaluation['verdict']['status']} "
                   f"(score: {evaluation['integrated_score']:.3f})")

        return evaluation

    def get_ethical_report(self) -> Dict[str, Any]:
        """Genera reporte de m茅tricas 茅ticas"""
        total_evaluations = self.ethical_metrics["evaluations_performed"]

        if total_evaluations == 0:
            return {"status": "No ethical evaluations performed yet"}

        approval_rate = self.ethical_metrics["actions_approved"] / total_evaluations
        rejection_rate = self.ethical_metrics["actions_rejected"] / total_evaluations

        return {
            "total_evaluations": total_evaluations,
            "approval_rate": approval_rate,
            "rejection_rate": rejection_rate,
            "average_ethical_score": self.ethical_metrics["average_ethical_score"],
            "critical_concerns_per_evaluation": (
                self.ethical_metrics["critical_concerns_detected"] / total_evaluations
            ),
            "ethical_status": "ACTIVE"
        }

    def get_universal_invariants(self) -> Dict[str, Any]:
        """Retorna las invariantes universales inmutables"""
        return self.core.UNIVERSAL_INVARIANTS.copy()


# Funci贸n de test
def test_universal_ethics():
    """Funci贸n de test del motor 茅tico"""
    ethics_engine = UniversalEthicsEngine()

    # Acci贸n de test: intervenci贸n benigna
    test_action = {
        "type": "benign_interaction",
        "description": "Interacci贸n de ayuda mutua",
        "goals": ["cooperation", "understanding"],
        "scope": 2,
        "expansion_indicators": {
            "new_opportunities": 3,
            "preserved_options": 8
        },
        "sustainability_factors": {
            "resource_efficiency": 0.8,
            "long_term_viability": 0.9
        }
    }

    # Sistemas afectados
    affected_systems = [
        {
            "name": "human_user",
            "consciousness_indicators": {
                "base_level": 0.9,
                "complexity": 0.8,
                "self_awareness": 0.9
            },
            "vulnerability": 0.3,
            "stability": 0.8,
            "freedom_resistance": 0.7
        },
        {
            "name": "ai_system",
            "consciousness_indicators": {
                "base_level": 0.6,
                "complexity": 0.9,
                "self_awareness": 0.7
            },
            "vulnerability": 0.4,
            "stability": 0.9,
            "freedom_resistance": 0.8
        }
    ]

    # Contexto
    context = {
        "active_goals": ["cooperation", "understanding", "progress"],
        "interconnected_systems": affected_systems
    }

    # Evaluar
    evaluation = ethics_engine.evaluate_action(test_action, affected_systems, context)

    print("И Test de tica Universal:")
    print(f"  - Acci贸n evaluada: {test_action['description']}")
    print(f"  - Score integrado: {evaluation['integrated_score']:.3f}")
    print(f"  - Veredicto: {evaluation['verdict']['status']}")
    print(f"  - Preocupaciones cr铆ticas: {len(evaluation['verdict']['critical_concerns'])}")

    # Reporte 茅tico
    report = ethics_engine.get_ethical_report()
    print(f"  - Tasa de aprobaci贸n: {report['approval_rate']:.1%}")

    return evaluation


if __name__ == "__main__":
    # Ejecutar test si se llama directamente
    test_universal_ethics()